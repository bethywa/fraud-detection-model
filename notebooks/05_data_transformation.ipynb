{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913d4607",
   "metadata": {},
   "source": [
    "# ğŸ““  05_data_transformation.ipynb (Scaling & Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bf978d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Starting data transformation...\n"
     ]
    }
   ],
   "source": [
    "# # Data Transformation\n",
    "# Scaling and encoding for modeling\n",
    "\n",
    "\n",
    "# ## 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ”„ Starting data transformation...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d856b9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Dataset shape: (129146, 48)\n",
      "ğŸ“‹ Features shape: (129146, 47)\n",
      "ğŸ¯ Target shape: (129146,)\n"
     ]
    }
   ],
   "source": [
    "# ## 2. Load Data with Features\n",
    "df = pd.read_csv('../data/processed/fraud_data_with_features.csv')\n",
    "print(f\"ğŸ“Š Dataset shape: {df.shape}\")\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('class', axis=1)\n",
    "y = df['class']\n",
    "\n",
    "print(f\"ğŸ“‹ Features shape: {X.shape}\")\n",
    "print(f\"ğŸ¯ Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2b3d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Identifying column types...\n",
      "ğŸ”¢ Numeric columns (37):\n",
      "['user_id', 'purchase_value', 'age', 'ip_address', 'purchase_hour', 'ip_address_int', 'lower_bound', 'upper_bound', 'is_high_risk_country', 'time_since_signup_hours'] ...\n",
      "\n",
      "ğŸ”¤ Categorical columns (10):\n",
      "['signup_time', 'purchase_time', 'device_id', 'source', 'browser', 'sex', 'purchase_day', 'country', 'purchase_day_name', 'age_group']\n",
      "\n",
      "ğŸ•’ Datetime columns (0):\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# ## 3. Identify Column Types\n",
    "print(\"ğŸ” Identifying column types...\")\n",
    "\n",
    "# Get column data types\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "datetime_cols = X.select_dtypes(include=['datetime64']).columns.tolist()\n",
    "\n",
    "print(f\"ğŸ”¢ Numeric columns ({len(numeric_cols)}):\")\n",
    "print(numeric_cols[:10], \"...\" if len(numeric_cols) > 10 else \"\")\n",
    "\n",
    "print(f\"\\nğŸ”¤ Categorical columns ({len(categorical_cols)}):\")\n",
    "print(categorical_cols)\n",
    "\n",
    "print(f\"\\nğŸ•’ Datetime columns ({len(datetime_cols)}):\")\n",
    "print(datetime_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a440f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ—‘ï¸  Dropping unnecessary columns...\n",
      "Dropped 12 columns:\n",
      "  - user_id\n",
      "  - device_id\n",
      "  - ip_address\n",
      "  - ip_address_int\n",
      "  - lower_bound\n",
      "  - upper_bound\n",
      "  - country\n",
      "  - signup_time\n",
      "  - purchase_time\n",
      "  - prev_purchase_time\n",
      "  - purchase_day_name\n",
      "  - age_group\n",
      "\n",
      "ğŸ“Š After dropping: (129146, 35)\n"
     ]
    }
   ],
   "source": [
    "# ## 4. Drop Unnecessary Columns\n",
    "print(\"\\nğŸ—‘ï¸  Dropping unnecessary columns...\")\n",
    "\n",
    "# Columns to drop (IDs, raw IPs, exact timestamps)\n",
    "cols_to_drop = [\n",
    "    'user_id', 'device_id', 'ip_address', 'ip_address_int',\n",
    "    'lower_bound', 'upper_bound', 'country',\n",
    "    'signup_time', 'purchase_time', 'prev_purchase_time',\n",
    "    'purchase_day_name', 'age_group'\n",
    "]\n",
    "\n",
    "# Only drop if they exist\n",
    "existing_cols_to_drop = [col for col in cols_to_drop if col in X.columns]\n",
    "X = X.drop(existing_cols_to_drop, axis=1)\n",
    "\n",
    "print(f\"Dropped {len(existing_cols_to_drop)} columns:\")\n",
    "for col in existing_cols_to_drop:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Update column lists after dropping\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nğŸ“Š After dropping: {X.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ece82384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¤ Encoding categorical variables...\n",
      "  âœ“ Encoded: source (3 unique values)\n",
      "  âœ“ Encoded: browser (5 unique values)\n",
      "  âœ“ Encoded: sex (2 unique values)\n",
      "  âœ“ Encoded: purchase_day (7 unique values)\n",
      "\n",
      "âœ… Encoded 4 categorical columns\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Handle Categorical Variables\n",
    "print(\"\\nğŸ”¤ Encoding categorical variables...\")\n",
    "\n",
    "# Create a copy for Label Encoding (simpler approach)\n",
    "X_encoded = X.copy()\n",
    "\n",
    "# Apply Label Encoding to categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    if col in X_encoded.columns:\n",
    "        # Handle NaN values first\n",
    "        X_encoded[col] = X_encoded[col].fillna('Unknown')\n",
    "        \n",
    "        # Apply label encoding\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "        \n",
    "        print(f\"  âœ“ Encoded: {col} ({len(le.classes_)} unique values)\")\n",
    "\n",
    "print(f\"\\nâœ… Encoded {len(label_encoders)} categorical columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1a76fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“ Scaling numerical features...\n",
      "Scaling 35 numerical columns\n",
      "âœ… Numerical features scaled\n"
     ]
    }
   ],
   "source": [
    "# ## 6. Scale Numerical Features\n",
    "print(\"\\nğŸ“ Scaling numerical features...\")\n",
    "\n",
    "# Identify numerical columns after encoding\n",
    "numeric_cols_final = X_encoded.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Remove target-related columns if they exist\n",
    "numeric_cols_final = [col for col in numeric_cols_final if col != 'class']\n",
    "\n",
    "print(f\"Scaling {len(numeric_cols_final)} numerical columns\")\n",
    "\n",
    "# Apply StandardScaler (good for most algorithms)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = X_encoded.copy()\n",
    "X_scaled[numeric_cols_final] = scaler.fit_transform(X_encoded[numeric_cols_final])\n",
    "\n",
    "print(\"âœ… Numerical features scaled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c73a47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”— Checking feature correlations...\n",
      "\n",
      "ğŸ“Š Top 10 features by absolute correlation:\n",
      "                    feature  correlation_with_target\n",
      "25         new_customer_1hr                 0.717107\n",
      "30        suspicious_device                 0.681906\n",
      "28         device_frequency                 0.671768\n",
      "29  unique_users_per_device                 0.671768\n",
      "26        new_customer_24hr                 0.666534\n",
      "27       new_customer_7days                 0.485032\n",
      "10           purchase_month                -0.313517\n",
      "8   time_since_signup_hours                -0.261308\n",
      "11    purchase_day_of_month                -0.163499\n",
      "7      is_high_risk_country                 0.046667\n",
      "\n",
      "âš ï¸  Highly correlated features (>0.8): ['avg_purchase_per_user', 'new_customer_24hr', 'device_frequency', 'unique_users_per_device', 'suspicious_device', 'high_risk_browser']\n"
     ]
    }
   ],
   "source": [
    "# ## 7. Feature Correlation Check\n",
    "print(\"\\nğŸ”— Checking feature correlations...\")\n",
    "\n",
    "# Calculate correlation with target\n",
    "correlations = pd.DataFrame({\n",
    "    'feature': X_scaled.columns,\n",
    "    'correlation_with_target': [np.corrcoef(X_scaled[col], y)[0, 1] \n",
    "                                for col in X_scaled.columns]\n",
    "}).sort_values('correlation_with_target', key=abs, ascending=False)\n",
    "\n",
    "print(\"\\nğŸ“Š Top 10 features by absolute correlation:\")\n",
    "print(correlations.head(10))\n",
    "\n",
    "# Check for highly correlated features (potential multicollinearity)\n",
    "correlation_matrix = X_scaled.corr().abs()\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "highly_correlated = [column for column in upper_triangle.columns \n",
    "                     if any(upper_triangle[column] > 0.8)]\n",
    "\n",
    "if highly_correlated:\n",
    "    print(f\"\\nâš ï¸  Highly correlated features (>0.8): {highly_correlated}\")\n",
    "else:\n",
    "    print(\"\\nâœ… No highly correlated features found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78716e7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ‚ï¸  Splitting data into train and test sets...\n",
      "âœ… Data split completed:\n",
      "   Training set: (103316, 35)\n",
      "   Test set: (25830, 35)\n",
      "   Training fraud rate: 9.4990%\n",
      "   Test fraud rate: 9.5006%\n"
     ]
    }
   ],
   "source": [
    "# ## 8. Split Data\n",
    "print(\"\\nâœ‚ï¸  Splitting data into train and test sets...\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split with stratification (preserve fraud ratio)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"âœ… Data split completed:\")\n",
    "print(f\"   Training set: {X_train.shape}\")\n",
    "print(f\"   Test set: {X_test.shape}\")\n",
    "print(f\"   Training fraud rate: {y_train.mean():.4%}\")\n",
    "print(f\"   Test fraud rate: {y_test.mean():.4%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0284c32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Saving transformed data...\n",
      "âœ… Data transformation completed!\n",
      "\n",
      "ğŸ“ Saved files:\n",
      "   - transformed_data.pkl\n",
      "   - X_train_transformed.csv\n",
      "   - X_test_transformed.csv\n",
      "\n",
      "ğŸ“Š Final feature count: 35\n"
     ]
    }
   ],
   "source": [
    "# ## 9. Save Transformed Data\n",
    "print(\"\\nğŸ’¾ Saving transformed data...\")\n",
    "\n",
    "import joblib\n",
    "\n",
    "# Save all components\n",
    "transformation_data = {\n",
    "    'X_train': X_train,\n",
    "    'X_test': X_test,\n",
    "    'y_train': y_train,\n",
    "    'y_test': y_test,\n",
    "    'feature_names': X_scaled.columns.tolist(),\n",
    "    'label_encoders': label_encoders,\n",
    "    'scaler': scaler,\n",
    "    'numeric_cols': numeric_cols_final,\n",
    "    'categorical_cols': categorical_cols\n",
    "}\n",
    "\n",
    "joblib.dump(transformation_data, '../data/processed/transformed_data.pkl')\n",
    "\n",
    "# Also save as CSV for easy inspection\n",
    "X_train_with_target = pd.concat([X_train, y_train.reset_index(drop=True)], axis=1)\n",
    "X_test_with_target = pd.concat([X_test, y_test.reset_index(drop=True)], axis=1)\n",
    "\n",
    "X_train_with_target.to_csv('../data/processed/X_train_transformed.csv', index=False)\n",
    "X_test_with_target.to_csv('../data/processed/X_test_transformed.csv', index=False)\n",
    "\n",
    "print(\"âœ… Data transformation completed!\")\n",
    "print(f\"\\nğŸ“ Saved files:\")\n",
    "print(\"   - transformed_data.pkl\")\n",
    "print(\"   - X_train_transformed.csv\")\n",
    "print(\"   - X_test_transformed.csv\")\n",
    "print(f\"\\nğŸ“Š Final feature count: {X_train.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
